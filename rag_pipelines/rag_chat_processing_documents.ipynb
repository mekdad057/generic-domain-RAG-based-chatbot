{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "600556e14b4d450a9d16014818ecc05f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cfedfcd3cc4f48ee9681db0dd9e6f3c7",
              "IPY_MODEL_0621df8fd62349b0b2224aa190afbe3c",
              "IPY_MODEL_2b6456876a2c4eacba4112bcdd94f3f3"
            ],
            "layout": "IPY_MODEL_a6ac64de882b4228a00536275837700a"
          }
        },
        "cfedfcd3cc4f48ee9681db0dd9e6f3c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9746e19e1d914c43a6e3dc9e61c62e0c",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_a56f9f78f9934c9398b72053955668b2",
            "value": "Batches:â€‡100%"
          }
        },
        "0621df8fd62349b0b2224aa190afbe3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_47e0ed3047cf437e9f5346003d00b492",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_caab9da8b63f45178c0b507d54723d89",
            "value": 5
          }
        },
        "2b6456876a2c4eacba4112bcdd94f3f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_82d356d996fc44fbb9e580d824a86d22",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_6ffb68e422d3480c835da1051cafea94",
            "value": "â€‡5/5â€‡[00:31&lt;00:00,â€‡â€‡2.93s/it]"
          }
        },
        "a6ac64de882b4228a00536275837700a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9746e19e1d914c43a6e3dc9e61c62e0c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a56f9f78f9934c9398b72053955668b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "47e0ed3047cf437e9f5346003d00b492": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "caab9da8b63f45178c0b507d54723d89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "82d356d996fc44fbb9e580d824a86d22": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ffb68e422d3480c835da1051cafea94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LsV81T8ghVDw"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet haystack-ai chroma-haystack\n",
        "!pip install --quiet --upgrade huggingface_hub\n",
        "!pip install --quiet python-docx"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y tesseract-ocr\n",
        "!pip install pytesseract opencv-python pillow\n",
        "!apt-get install -y tesseract-ocr-ara\n",
        "\n",
        "!pip install pdf2image\n",
        "!apt-get install -y poppler-utils  # required for pdf2image"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SugwgWLwlOz6",
        "outputId": "92b8a8bb-4339-4356-b499-1d78186c90ed"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n",
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.11/dist-packages (0.3.13)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.3.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (25.0)\n",
            "Requirement already satisfied: numpy<2.3.0,>=2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (2.0.2)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tesseract-ocr-ara is already the newest version (1:4.00~git30-7274cfa-1.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n",
            "Requirement already satisfied: pdf2image in /usr/local/lib/python3.11/dist-packages (1.17.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from pdf2image) (11.3.0)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "poppler-utils is already the newest version (22.02.0-2ubuntu0.9).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack import component\n",
        "from haystack import Pipeline, Document\n",
        "from haystack.components.preprocessors import DocumentCleaner, DocumentSplitter\n",
        "from haystack.components.writers import DocumentWriter\n",
        "from haystack.document_stores.types import DuplicatePolicy\n",
        "from haystack_integrations.document_stores.chroma import ChromaDocumentStore\n",
        "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
        "from haystack.components.embedders import HuggingFaceAPIDocumentEmbedder\n",
        "from haystack.components.embedders import SentenceTransformersDocumentEmbedder\n",
        "from haystack.utils import Secret\n",
        "\n",
        "from tqdm.notebook import tqdm"
      ],
      "metadata": {
        "id": "C-FFbVX2h_7x"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from typing import List\n",
        "\n",
        "from haystack import Document, component\n",
        "from pdf2image import convert_from_path\n",
        "import pytesseract\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "@component\n",
        "class PdfToSingleDocumentConverter:\n",
        "    \"\"\"\n",
        "    A component that converts multiple PDF files to Document objects using OCR.\n",
        "\n",
        "    This component processes multiple PDF files and creates a Document object for each one.\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def validate(pdf_path: str):\n",
        "        \"\"\"Validate that the PDF file exists and required dependencies are available.\"\"\"\n",
        "        if not os.path.exists(pdf_path):\n",
        "            raise ValueError(f\"PDF file '{pdf_path}' does not exist.\")\n",
        "\n",
        "        # Check if Tesseract is available\n",
        "        try:\n",
        "            pytesseract.get_tesseract_version()\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(\n",
        "                \"Tesseract OCR is not properly installed or configured. \"\n",
        "                \"Please install it and ensure it's in your PATH. \"\n",
        "                \"For Arabic support, also install the Arabic language pack: tesseract-ocr-ara\"\n",
        "            ) from e\n",
        "\n",
        "    @staticmethod\n",
        "    def preprocess_image(pil_image):\n",
        "        \"\"\"Preprocess an image to improve OCR quality.\"\"\"\n",
        "        image = np.array(pil_image)\n",
        "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "        _, thresh = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "        return thresh\n",
        "\n",
        "    @staticmethod\n",
        "    def pdf_preprocess(text: str) -> str:\n",
        "        \"\"\"Clean and preprocess extracted text.\"\"\"\n",
        "        text = text.replace('\\r', '')\n",
        "        text = text.split('\\n\\n')\n",
        "        text = [i.replace('\\n', ' ').strip() for i in text]\n",
        "        text = [i for i in text if i != '']\n",
        "        text = '\\n\\n'.join(text)\n",
        "        return text\n",
        "\n",
        "    @component.output_types(documents=List[Document])\n",
        "    def run(self, pdf_paths: List[str], dpi: int = 300, language: str = 'ara'):\n",
        "        \"\"\"\n",
        "        Convert multiple PDF files to Document objects.\n",
        "\n",
        "        Args:\n",
        "            pdf_paths: List of paths to PDF files to process.\n",
        "            dpi: DPI for image conversion (higher = better quality but slower).\n",
        "            language: Tesseract language code (default 'ara' for Arabic).\n",
        "\n",
        "        Returns:\n",
        "            A dictionary with a 'documents' key containing Document objects for all PDFs.\n",
        "        \"\"\"\n",
        "        all_documents = []\n",
        "\n",
        "        for pdf_path in pdf_paths:\n",
        "            self.validate(pdf_path)\n",
        "\n",
        "            # Convert PDF to images\n",
        "            pages = convert_from_path(pdf_path, dpi=dpi)\n",
        "\n",
        "            # Process each page\n",
        "            extracted_texts = []\n",
        "            for page in pages:\n",
        "                # Preprocess image\n",
        "                processed_img = self.preprocess_image(page)\n",
        "\n",
        "                # Extract text with OCR\n",
        "                pil_img = Image.fromarray(processed_img)\n",
        "                text = pytesseract.image_to_string(pil_img, lang=language)\n",
        "                extracted_texts.append(text)\n",
        "\n",
        "            # Clean and process extracted text\n",
        "            processed_texts = [self.pdf_preprocess(text) for text in extracted_texts]\n",
        "\n",
        "            # Join all pages into a single text\n",
        "            full_text = \"\\n\\n\".join(processed_texts)\n",
        "\n",
        "            # Create a single Document object for the entire PDF\n",
        "            doc = Document(\n",
        "                content=full_text,\n",
        "                meta={\n",
        "                    \"file_path\": pdf_path,\n",
        "                    \"total_pages\": len(processed_texts),\n",
        "                    \"language\": language,\n",
        "                    \"is_full_document\": True\n",
        "                }\n",
        "            )\n",
        "\n",
        "            all_documents.append(doc)\n",
        "\n",
        "        return {\"documents\": all_documents}"
      ],
      "metadata": {
        "id": "QDLupN6Zh3xp"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import List\n",
        "\n",
        "from haystack import Document, component\n",
        "import docx\n",
        "\n",
        "\n",
        "@component\n",
        "class WordToDocumentConverter:\n",
        "    \"\"\"\n",
        "    A component that converts multiple Word documents to Document objects.\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def validate(word_path: str):\n",
        "        \"\"\"Validate that the Word file exists and has a supported format.\"\"\"\n",
        "        if not os.path.exists(word_path):\n",
        "            raise ValueError(f\"Word file '{word_path}' does not exist.\")\n",
        "\n",
        "        if not word_path.lower().endswith(('.docx', '.doc')):\n",
        "            raise ValueError(\"Only DOCX and DOC files are supported.\")\n",
        "\n",
        "    @component.output_types(documents=List[Document])\n",
        "    def run(self, word_paths: List[str]):\n",
        "        \"\"\"\n",
        "        Convert multiple Word documents to Document objects.\n",
        "\n",
        "        Args:\n",
        "            word_paths: List of paths to Word documents to process.\n",
        "\n",
        "        Returns:\n",
        "            A dictionary with a 'documents' key containing Document objects for all Word files.\n",
        "        \"\"\"\n",
        "        all_documents = []\n",
        "\n",
        "        for word_path in word_paths:\n",
        "            self.validate(word_path)\n",
        "\n",
        "            # Extract text from Word document\n",
        "            document = docx.Document(word_path)\n",
        "            full_text = []\n",
        "\n",
        "            for para in document.paragraphs:\n",
        "                full_text.append(para.text)\n",
        "\n",
        "            # Process tables if needed\n",
        "            for table in document.tables:\n",
        "                for row in table.rows:\n",
        "                    for cell in row.cells:\n",
        "                        full_text.append(cell.text)\n",
        "\n",
        "            full_text = \"\\n\\n\".join(full_text)\n",
        "\n",
        "            # Create a Document object\n",
        "            doc = Document(\n",
        "                content=full_text,\n",
        "                meta={\n",
        "                    \"file_path\": word_path,\n",
        "                    \"file_type\": \"word\",\n",
        "                    \"total_paragraphs\": len(document.paragraphs),\n",
        "                    \"total_tables\": len(document.tables)\n",
        "                }\n",
        "            )\n",
        "\n",
        "            all_documents.append(doc)\n",
        "\n",
        "        return {\"documents\": all_documents}"
      ],
      "metadata": {
        "id": "cNDOycnDoQcw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import List\n",
        "\n",
        "from haystack import Document, component\n",
        "\n",
        "\n",
        "@component\n",
        "class TextFileToDocumentConverter:\n",
        "    \"\"\"\n",
        "    A component that converts multiple text files to Document objects.\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def validate(text_path: str):\n",
        "        \"\"\"Validate that the text file exists and has a supported format.\"\"\"\n",
        "        if not os.path.exists(text_path):\n",
        "            raise ValueError(f\"Text file '{text_path}' does not exist.\")\n",
        "\n",
        "        if not text_path.lower().endswith('.txt'):\n",
        "            raise ValueError(\"Only TXT files are supported.\")\n",
        "\n",
        "    @component.output_types(documents=List[Document])\n",
        "    def run(self, text_paths: List[str], encoding: str = 'utf-8'):\n",
        "        \"\"\"\n",
        "        Convert multiple text files to Document objects.\n",
        "\n",
        "        Args:\n",
        "            text_paths: List of paths to text files to process.\n",
        "            encoding: File encoding (default 'utf-8').\n",
        "\n",
        "        Returns:\n",
        "            A dictionary with a 'documents' key containing Document objects for all text files.\n",
        "        \"\"\"\n",
        "        all_documents = []\n",
        "\n",
        "        for text_path in text_paths:\n",
        "            self.validate(text_path)\n",
        "\n",
        "            # Read text from file\n",
        "            try:\n",
        "                with open(text_path, 'r', encoding=encoding) as file:\n",
        "                    full_text = file.read()\n",
        "            except Exception as e:\n",
        "                raise RuntimeError(f\"Failed to read text file: {str(e)}\") from e\n",
        "\n",
        "            # Create a Document object\n",
        "            doc = Document(\n",
        "                content=full_text,\n",
        "                meta={\n",
        "                    \"file_path\": text_path,\n",
        "                    \"file_type\": \"text\",\n",
        "                    \"encoding\": encoding,\n",
        "                    \"character_count\": len(full_text)\n",
        "                }\n",
        "            )\n",
        "\n",
        "            all_documents.append(doc)\n",
        "\n",
        "        return {\"documents\": all_documents}"
      ],
      "metadata": {
        "id": "XxZocmF4oo2v"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import List\n",
        "\n",
        "from haystack import Document, component\n",
        "\n",
        "\n",
        "@component\n",
        "class FileTypeDetector:\n",
        "    \"\"\"\n",
        "    A component that detects file types based on extensions and groups them by type.\n",
        "\n",
        "    This component examines file extensions and groups file paths\n",
        "    into categories: PDF, Word, and text files.\n",
        "\n",
        "    Example usage:\n",
        "\n",
        "    ```python\n",
        "    detector = FileTypeDetector()\n",
        "    result = detector.run(file_paths=[\"/path/to/doc1.pdf\", \"/path/to/doc2.docx\"])\n",
        "    # Returns with paths grouped by type\n",
        "    ```\n",
        "    \"\"\"\n",
        "\n",
        "    @component.output_types(pdf_paths=List[str], word_paths=List[str], text_paths=List[str])\n",
        "    def run(self, file_paths: List[str]):\n",
        "        \"\"\"\n",
        "        Group file paths by type.\n",
        "\n",
        "        Args:\n",
        "            file_paths: List of paths to files to be processed\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with paths grouped by file type\n",
        "        \"\"\"\n",
        "        pdf_paths = []\n",
        "        word_paths = []\n",
        "        text_paths = []\n",
        "\n",
        "        for file_path in file_paths:\n",
        "            # Validate file exists\n",
        "            if not os.path.exists(file_path):\n",
        "                raise ValueError(f\"File not found: {file_path}\")\n",
        "\n",
        "            _, extension = os.path.splitext(file_path)\n",
        "            extension = extension.lower()\n",
        "\n",
        "            if extension == '.pdf':\n",
        "                pdf_paths.append(file_path)\n",
        "            elif extension in ['.docx', '.doc']:\n",
        "                word_paths.append(file_path)\n",
        "            elif extension == '.txt':\n",
        "                text_paths.append(file_path)\n",
        "            else:\n",
        "                raise ValueError(f\"Unsupported file format: {extension}. Supported formats are PDF, DOCX, DOC, and TXT.\")\n",
        "\n",
        "        return {\n",
        "            \"pdf_paths\": pdf_paths,\n",
        "            \"word_paths\": word_paths,\n",
        "            \"text_paths\": text_paths\n",
        "        }"
      ],
      "metadata": {
        "id": "BhXIHiUxqP4F"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Dict, Any, Optional\n",
        "\n",
        "from haystack import Document, component\n",
        "\n",
        "\n",
        "@component\n",
        "class DocumentMerger:\n",
        "    \"\"\"\n",
        "    A component that merges documents from multiple sources into a single list.\n",
        "\n",
        "    This component collects documents from PDF, Word, and text converters\n",
        "    and combines them into a single list for downstream processing.\n",
        "    \"\"\"\n",
        "\n",
        "    @component.output_types(documents=List[Document])\n",
        "    def run(self, pdf_documents: List[Document] = [],\n",
        "            word_documents: List[Document] = [],\n",
        "            text_documents: List[Document] = []):\n",
        "        \"\"\"\n",
        "        Merge documents from different sources.\n",
        "\n",
        "        Args:\n",
        "            pdf_documents: Documents from PDF converter\n",
        "            word_documents: Documents from Word converter\n",
        "            text_documents: Documents from text converter\n",
        "\n",
        "        Returns:\n",
        "            Merged list of documents\n",
        "        \"\"\"\n",
        "        all_documents = []\n",
        "\n",
        "        if pdf_documents:\n",
        "            all_documents.extend(pdf_documents)\n",
        "        if word_documents:\n",
        "            all_documents.extend(word_documents)\n",
        "        if text_documents:\n",
        "            all_documents.extend(text_documents)\n",
        "\n",
        "        return {\"documents\": all_documents}"
      ],
      "metadata": {
        "id": "VdWU_Xc4q9_I"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack import Pipeline\n",
        "from haystack.components.preprocessors import DocumentCleaner, DocumentSplitter\n",
        "from haystack.components.writers import DocumentWriter\n",
        "from haystack.document_stores.types import DuplicatePolicy\n",
        "from haystack_integrations.document_stores.chroma import ChromaDocumentStore\n",
        "from haystack.components.embedders import SentenceTransformersDocumentEmbedder\n",
        "\n",
        "# Initialize document store (using Chroma as in your example)\n",
        "document_store = ChromaDocumentStore(\n",
        "    embedding_function='default',\n",
        "    persist_path='/content/vectordb'\n",
        ")\n",
        "\n",
        "# Embedding model configuration\n",
        "embedder_name = \"sayed0am/arabic-english-bge-m3\"\n",
        "\n",
        "# Create the unified pipeline\n",
        "pipeline = Pipeline()\n",
        "\n",
        "# Add components for file type detection and conversion\n",
        "pipeline.add_component('file_type_detector', FileTypeDetector())\n",
        "pipeline.add_component('pdf_converter', PdfToSingleDocumentConverter())\n",
        "pipeline.add_component('word_converter', WordToDocumentConverter())\n",
        "pipeline.add_component('text_converter', TextFileToDocumentConverter())\n",
        "pipeline.add_component('document_merger', DocumentMerger())\n",
        "pipeline.add_component('embedder', SentenceTransformersDocumentEmbedder(model=embedder_name))\n",
        "\n",
        "# Add standard processing components\n",
        "pipeline.add_component('cleaner', DocumentCleaner(\n",
        "    remove_empty_lines=True,\n",
        "    remove_extra_whitespaces=True,\n",
        "    remove_repeated_substrings=False\n",
        "))\n",
        "pipeline.add_component('splitter', DocumentSplitter(\n",
        "    split_by='sentence',\n",
        "    split_length=3,\n",
        "    split_overlap=0\n",
        "))\n",
        "pipeline.add_component('writer', DocumentWriter(\n",
        "    document_store=document_store,\n",
        "    policy=DuplicatePolicy.SKIP\n",
        "))\n",
        "\n",
        "# Connect file type detector to appropriate converters\n",
        "pipeline.connect('file_type_detector.pdf_paths', 'pdf_converter.pdf_paths')\n",
        "pipeline.connect('file_type_detector.word_paths', 'word_converter.word_paths')\n",
        "pipeline.connect('file_type_detector.text_paths', 'text_converter.text_paths')\n",
        "\n",
        "# Connect converters to the document merger\n",
        "pipeline.connect('pdf_converter.documents', 'document_merger.pdf_documents')\n",
        "pipeline.connect('word_converter.documents', 'document_merger.word_documents')\n",
        "pipeline.connect('text_converter.documents', 'document_merger.text_documents')\n",
        "\n",
        "# Connect the rest of the pipeline\n",
        "pipeline.connect('document_merger.documents', 'cleaner.documents')\n",
        "pipeline.connect('cleaner.documents', 'splitter.documents')\n",
        "pipeline.connect('splitter.documents', 'embedder.documents')\n",
        "pipeline.connect('embedder.documents', 'writer.documents')"
      ],
      "metadata": {
        "id": "JuwOm1zjnGw7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8501614a-175a-48f2-9b9d-74ce4f6fd6f5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<haystack.core.pipeline.pipeline.Pipeline object at 0x7d3aacfb7e90>\n",
              "ðŸš… Components\n",
              "  - file_type_detector: FileTypeDetector\n",
              "  - pdf_converter: PdfToSingleDocumentConverter\n",
              "  - word_converter: WordToDocumentConverter\n",
              "  - text_converter: TextFileToDocumentConverter\n",
              "  - document_merger: DocumentMerger\n",
              "  - embedder: SentenceTransformersDocumentEmbedder\n",
              "  - cleaner: DocumentCleaner\n",
              "  - splitter: DocumentSplitter\n",
              "  - writer: DocumentWriter\n",
              "ðŸ›¤ï¸ Connections\n",
              "  - file_type_detector.pdf_paths -> pdf_converter.pdf_paths (List[str])\n",
              "  - file_type_detector.word_paths -> word_converter.word_paths (List[str])\n",
              "  - file_type_detector.text_paths -> text_converter.text_paths (List[str])\n",
              "  - pdf_converter.documents -> document_merger.pdf_documents (List[Document])\n",
              "  - word_converter.documents -> document_merger.word_documents (List[Document])\n",
              "  - text_converter.documents -> document_merger.text_documents (List[Document])\n",
              "  - document_merger.documents -> cleaner.documents (List[Document])\n",
              "  - embedder.documents -> writer.documents (List[Document])\n",
              "  - cleaner.documents -> splitter.documents (List[Document])\n",
              "  - splitter.documents -> embedder.documents (List[Document])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Process a PDF file\n",
        "paths_list = ['/content/pdddf.pdf','/content/tttext.txt']\n",
        "indexing_results = pipeline.run({\n",
        "    'file_type_detector': {'file_paths': paths_list},\n",
        "}, include_outputs_from={'pdf_converter', 'word_converter', 'text_converter'})"
      ],
      "metadata": {
        "id": "4Xm4GR9-pXGY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "600556e14b4d450a9d16014818ecc05f",
            "cfedfcd3cc4f48ee9681db0dd9e6f3c7",
            "0621df8fd62349b0b2224aa190afbe3c",
            "2b6456876a2c4eacba4112bcdd94f3f3",
            "a6ac64de882b4228a00536275837700a",
            "9746e19e1d914c43a6e3dc9e61c62e0c",
            "a56f9f78f9934c9398b72053955668b2",
            "47e0ed3047cf437e9f5346003d00b492",
            "caab9da8b63f45178c0b507d54723d89",
            "82d356d996fc44fbb9e580d824a86d22",
            "6ffb68e422d3480c835da1051cafea94"
          ]
        },
        "outputId": "6d0d8e86-5833-450c-c7a2-4efda6df11f1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "600556e14b4d450a9d16014818ecc05f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `XLMRobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "indexing_results.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUCx9bsr6zOi",
        "outputId": "b60e5dd5-b464-466a-9019-d78e49604816"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['pdf_converter', 'text_converter', 'word_converter', 'writer'])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# storing vectordb in drive! check vectordb name before saving\n",
        "\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# Define paths\n",
        "doc_store_path = '/content/vectordb'  # Change this to your actual path\n",
        "zip_file_name = 'vectordb-aren-sayed0am-testing.zip'\n",
        "drive_path = '/content/'  # Or a specific folder in Drive\n",
        "\n",
        "# Zip the folder\n",
        "shutil.make_archive(os.path.join(drive_path, zip_file_name.replace('.zip','')), 'zip', doc_store_path)\n",
        "\n",
        "print(f\"Zipped document store saved to Google Drive at {drive_path}{zip_file_name}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "inGcu6sx62Ak",
        "outputId": "6bbcbc94-45b6-499f-ddf2-1c689890ec48"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zipped document store saved to Google Drive at /content/vectordb-aren-sayed0am-testing.zip\n"
          ]
        }
      ]
    }
  ]
}